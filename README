Neurohex uses Deep Q-learning with a convolutional neural network to learn to play the game of hex through self-play. It is written in python using theano and some lasagne.

mentor.py contains a script which preforms supervised learning over a prescored dataset in order to give the network a decent initialization before q_learning.

q_learn.py contains a script which trains a network by deep Q-learning through self play.

The directory playerAgents contains program.py which is an executable hex agent that makes use of a trained network which is also included. The included network is inspired by the arcitecture of the value network of Google DeepMind's alphaGo. It was trained by first mentoring a version of a common hex heuristic based on electircal resistance over a dataset generated by a strong hexplayer called wolve (see https://sourceforge.net/projects/benzene/) and then training by selfplay. program.py communicates using the gtp-protocol (https://www.lysator.liu.se/~gunnar/gtp/) and can be played against using an interface like hexgui (https://github.com/ryanbhayward/hexgui), or simply by typing gtp commands via command line. Plays best on 13x13 as this is all it was trained for, however it should now be able to play on any *ODD* boardsize (but not even) where it will simply see the extra cells as filled in appropriately.

To use the code it is nessesary to install numpy and theano.

A paper on Neurohex can be found here: http://arxiv.org/abs/1604.07097
